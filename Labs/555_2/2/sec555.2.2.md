Lab 2.2 - Investigating HTTP
==========================================================

Objectives
==========

-   Use standard HTTP fields to find abnormal events

-   Use log enrichment data to filter out noise

-   Identify web server scans

-   Look for unusual naked IP requests

-   Learn to build and use visualizations and dashboards

Exercise Preparation
====================

Log into the Sec-555 VM

-   Username: student

-   Password: sec555

<img src="./media/image1.png" width="609" height="156" />

Exercise: No hints
==================

Logs for this lab have already been ingested and are stored in index **pcap-\*** and have a **type** of **http**. To answer the questions below use Kibana.

1.  Between **March 10<sup>th</sup>** and **March 15<sup>th</sup>** of **2017** multiple web scans were performed against **vmmonitor.test.int** and **pki01.test.int**. This activity included attempts to perform directory traversals, cross site scripting, and many other forms of web attacks.

    1.  Which system performed the scan? 10.0.1.201

    2.  How many 404 errors were caused by this scan? 12,192

    3.  Was the scan malicious? No. It was a Nessus scan.

2.  Starting in **2017**, Lab Me Inc. began monitoring naked IP requests. These are tagged with **naked\_ip**. Specifically, they are monitoring outbound connections from **192.168.2.0/24** and **10.0.0.0/24**. Outbound connections from these subnets are being monitored as they go out to the internet through a **Fortinet** firewall.

    1.  There are many naked IP requests. Filtering which two ASNs eliminate almost all the naked IP requests? Fortinet, Inc. and Netflix Streaming Services Inc.

    2.  Are there naked IP requests to other common businesses? Google and Amazon

    3.  After eliminating common ASNs, how many naked IP addresses need investigated?

Exercise â€“ Step-by-step instructions
====================================

First open Kibana and change the index to **pcap-\***.

<img src="./media/image2.png" width="314" height="101" />

1. Identify the web scan that occurred between March 10<sup>th</sup> and March 15<sup>th</sup> of 2017
---------
First, click on the **date picker** in the top right corner and then click on **Absolute**. Set the **From** field to **2017-03-10** and the **To** field to **2017-03-15 23:59**.

<img src="./media/image3.png" width="675" height="236" />

**Note**: Notice that the **From** and **To** fields do not have to be filled out. If you do not specific something a zero is submitted in its place. For example, the **From** field for this search was changed to **2017-03-10 00:00:00.000**.

This shows **118,084** hits. Visually there are some spikes but the **Discover** tab is not the best way to track the web scan down. Switch to the **Visualize** tab and then click on the **New Visualization** icon. Then click on **Vertical Bar Chart**.

<img src="./media/image4.png" width="667" height="82" />

<img src="./media/image5.png" width="647" height="136" />

Select **From a new search** and then select **pcap-\*.**

<img src="./media/image6.png" width="398" height="200" />

When looking for web scan activity it is common to see a status code of 404 as well as 200. Start by building out a bar chart that identifies the top sources of 404 errors by source IP. In the search bar enter "**type:http AND status\_code:404**". Then click on the search icon.

```bash
type:http AND status_code:404
```

<img src="./media/image7.png" width="326" height="42" />

Next, click on Split Bars.

<img src="./media/image8.png" width="179" height="153" />

Then set the **Aggregation** to **Terms**, **Field** to **source\_ip**, **Size** to **3**, and **CustomLabel** to **Source IP**. Then click on **Add sub-buckets**.

<img src="./media/image9.png" width="384" height="468" />

Select X-Axis. Then set Sub Aggregation to Date Histogram. Then click on the green play button.

<img src="./media/image10.png" width="207" height="107" />

<img src="./media/image11.png" width="385" height="195" />

**Note**: The reason for adding the **Split Bars** before the **X-Axis** is so that the search is first sorted by **source\_ip** and then by **date**. This makes the legend reflect the top **source\_ip** that has 404 status codes. If you were to add the **X-Axis** first and then the **Split Bars** the outcome would display differently due to the order of the search.

Save the visualization by clicking on the save icon. Set the **Title** to **HTTP Status Code 404 by Source IP** and then click on **Save**.

<img src="./media/image12.png" width="512" height="167" />

You should then see the below graph.

<img src="./media/image13.png" width="661" height="286" />

**10.0.1.201** has by far more 404 status codes than any other IP. At this point it is likely that **10.0.1.201** is scanning one or more systems. Before diving more into this modify the chart to be for a status code of 200. Do this by changing the search bar to have "**type:http AND status\_code:200**" and then click on the search icon.

```bash
type:http AND status_code:200
```

<img src="./media/image14.png" width="331" height="39" />

In this graph **10.0.0.1** has the most 200 status codes followed by **10.0.1.201**. This means that 10.0.0.1 could also be performing a web scan. However, it is unclear without knowing the destination **virtual\_host(s)**.

<img src="./media/image15.png" width="664" height="286" />

You now have the same chart layout but specific to web page requests that were successful. Click on the save icon and change the **Title** to **HTTP Status Code 200 by Source IP** and then click on **Save**.

<img src="./media/image16.png" width="504" height="169" />

While 10.0.1.201 still looks like the primary suspect these charts do not identify the target web site(s). To identify these targets, create a new visualization by clicking on the **New Visualization** button.

<img src="./media/image17.png" width="420" height="82" />

This time select **Data table**.

<img src="./media/image18.png" width="443" height="193" />

Select **From a new search** and then select **pcap-\*.**

<img src="./media/image6.png" width="398" height="200" />

First, set the search filter to "**type:http**". This makes the visualization specific to only **http** events.

```bash
type:http
```

<img src="./media/image19.png" width="270" height="40" />

For the bucket type select **Split Rows**.

<img src="./media/image20.png" width="168" height="106" />

Set **Aggregation** to **Terms**, **Field** to **method.raw**, and **CustomLabel** to **Method**. Then click on **Add sub-buckets**.

<img src="./media/image21.png" width="387" height="469" />

For the bucket type select **Split Rows**.

<img src="./media/image20.png" width="168" height="106" />

Set **Sub Aggregation** to **Terms**, **Field** to **virtual\_host.raw**, **Size** to **2**, and **CustomLabel** to **Virtual Host**. Then click on **Add sub-buckets**.

<img src="./media/image22.png" width="374" height="435" />

For the bucket type select **Split Rows**.

<img src="./media/image20.png" width="168" height="106" />

Set **Sub Aggregation** to **Date Histogram** and then click on the green play button.

<img src="./media/image23.png" width="380" height="173" />

**Note**: The remaining question for step one is what are the target web servers that were scanned. This only requires the **virtual\_host.raw** field. However, adding the **method.raw** and **Date Histogram** into the visualization allows it to be multi-purpose. For instance, it could be used to find abnormal GET vs POST use. Also, the timestamp breakdown helps identify if this is many events over a short period or many events simply because you are searching over a long period.

Save the visualization by clicking on the save icon. Set the **Title** to **HTTP Methods by Virtual Host** and then click **Save**.

<img src="./media/image24.png" width="458" height="168" />

Before adding these visualizations to a dashboard switch back to the **Discover** tab and create a saved search for HTTP. First click on the **Discover** tab. Then search for "**type:http**".

<img src="./media/image25.png" width="280" height="43" />

Then hover over the following fields in the left column and click on Add to add them as columns:

- method
- virtual\_host
- uri
- uri\_length
- destination\_geo.asn
- destination\_geo.country\_name

Example using method:

<img src="./media/image26.png" width="311" height="118" />

Then click on the save icon and set the **Save Search** title to **HTTP**. Then click on **Save**.

<img src="./media/image27.png" width="433" height="198" />

Now create a dashboard and add these three visualizations and saved search to it. Switch to the **Dashboard** tab. Then click on the **New Dashboard** icon.

<img src="./media/image28.png" width="438" height="86" />

Click on the Add Visualization icon and then add **HTTP Methods by Virtual Host**, **HTTP Status Code 200 by Source IP**, and **HTTP Status Code 404 by Source IP** by clicking on them.

<img src="./media/image29.png" width="359" height="235" />

Then switch to the **Searches** tab and add **HTTP**. Then minimize the **Add Visualization** view by clicking the up arrow at the bottom.

<img src="./media/image30.png" width="692" height="229" />

Rearrange the visualizations to your liking. Throughout this process the time should still be set to **March 10<sup>th</sup>** to **March 15<sup>th</sup>**. The dashboard reflects this by showing relative times such as X months ago or X years ago. If the date is not accurate set it to March 10<sup>th</sup> to March 15<sup>th</sup> again. Save the dashboard by clicking on the Save icon and setting the **Save As** title to **HTTP Dashboard** and then click **Save**.

Now click on **10.0.1.201** on the **HTTP Status Code 404 by Source IP** visualization and then click on the magnifying glass with the + sign. This will apply a search filter of **source\_ip:"10.0.1.201"** to the dashboard.

<img src="./media/image31.png" width="628" height="354" />

Now the dashboard only reflects activity from a source IP address of 10.0.1.201. However, the first question specifically is asking about **vmmonitor.test.int** and **pki01.test.int**. To have the dashboard filter down on these systems search for "**type:http AND (virtual\_host:"vmmonitor.test.int" OR virtual\_host:"pki01.test.int")**" in the search bar.

```bash
type:http AND (virtual_host:"vmmonitor.test.int" OR virtual_host:"pki01.test.int")
```
<img src="./media/image32.png" width="664" height="79" />

The search results show many GET and POST requests against both of these target sites. The question remaining is whether this is a malicious scan or not. This can be difficult to identify especially if you do not know if there are trusted scanners at your organization. Within the saved search click on the **Time** column to sort the Time in Chronological order.

<img src="./media/image33.png" width="452" height="181" />

This should show that the first event from 10.0.1.201 occurred on March 11<sup>th</sup> 2017 at 02:01:26.761.

**Note**: This timestamp is specific to Central Time. The time of the logs will be shown according to the time zone of the machine accessing Kibana.

<img src="./media/image34.png" width="477" height="189" />

Occasionally, an **useragent** will give away whether something is malicious or not. However, looking at any of these logs shows that 10.0.1.201 has an **useragent** of **Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0).** This is not helpful as it reflects a Windows XP system using Internet Explorer 8. The next step could be to analyze the **uri** field to see if there are any telltale signs.

Go ahead and scroll through a few pages of the saved search results and look at the uri field. Switch to page 5.

<img src="./media/image35.png" width="667" height="127" />

The first two requests are directory traversal attacks trying to access **win.ini**. Scrolling down to a little over halfway down the page you will find the following:

<img src="./media/image36.png" width="669" height="69" />

This time the directory traversal attack starts with **/nessus**. This gives away that the scan is being performed by a Nessus vulnerability scanner.

**Answer**: **10.0.1.201** is likely not a malicious system. It is a vulnerability scanner. Between March 10<sup>th</sup> and March 15<sup>th,</sup> it caused **4,504** status code 404 errors against **vmmonitor.test.int** and **pki01.test.int**. The 404 count can be calculated by hovering over the bars in the **HTTP Status Code 404 by Source IP** visualization and adding the totals.

2. Find out which naked IP requests can be filtered and which need investigated
---------
**Note**: A naked IP request is a web request to an IP address rather than a domain name.

This question involves looking for naked IP requests and finding ways to filter our legitimate noise. This requires finding naked IP requests as well as information that can be used to filter on. For this lab, a **tag** of **naked\_ip** has been added to any HTTP event that has a **virtual\_host** using an IP address.

Begin by switching to the **Visualize** tab and then click on the **New Visualization** icon.

<img src="./media/image37.png" width="596" height="118" />

Select **Data table**.

<img src="./media/image18.png" width="443" height="193" />

Select **From a new search** and then select **pcap-\*.**

<img src="./media/image6.png" width="398" height="200" />

First, set the search filter to "**type:http AND tags:naked\_ip**". This makes the visualization specific to only **http** events.

```bash
type:http AND tags:naked_ip
```

<img src="./media/image38.png" width="349" height="39" />

For the bucket type select **Split Rows**.

<img src="./media/image20.png" width="168" height="106" />

Set **Aggregation** to **Terms**, **Field** to **virtual\_host.raw**, **Size** to **6**, and **CustomLabel** to **Virtual Host**. Then click on **Add sub-buckets**.

<img src="./media/image39.png" width="383" height="466" />

For the bucket type select **Split Rows**.

<img src="./media/image20.png" width="168" height="106" />

Set **Sub Aggregation** to **Terms**, **Field** to **reverse\_dns.raw**, and **CustomLabel** to **Reverse DNS**. Then click on the green play button.

<img src="./media/image40.png" width="379" height="463" />

**Note**: Reverse DNS takes an IP address and attempts to resolve it back to a domain. Some sites do not have reverse DNS entries. However, when they do it is very helpful to identify and filter out legitimate traffic.

<img src="./media/image41.png" width="613" height="218" />

Save the visualization by clicking on the save icon. Set the **Title** to **HTTP Naked IPs** and then click on **Save**.

<img src="./media/image42.png" width="519" height="164" />

ASN and geo information is extremely helpful for filtering out legitimate naked IP requests. Proceed by building a visualization with this information. Click on the **New Visualization** icon.

<img src="./media/image43.png" width="596" height="118" />

Select **Data table**.

<img src="./media/image18.png" width="443" height="193" />

Select **From a new search** and then select **pcap-\*.**

<img src="./media/image6.png" width="398" height="200" />

Search for "**type:http**".

```bash
type:http
```

<img src="./media/image25.png" width="280" height="43" />

For the bucket type select **Split Rows**.

<img src="./media/image20.png" width="168" height="106" />

Set **Aggregation** to **Terms**, **Field** to **destination\_geo.asn.raw**, and **CustomLabel** to **ASN**. Then click on **Add sub-buckets**.

<img src="./media/image44.png" width="373" height="462" />

For the bucket type select **Split Rows**.

<img src="./media/image20.png" width="168" height="106" />

Set **Sub Aggregation** to **Terms**, **Field** to **destination\_geo.country\_name.raw**, **Size** to **3**, and **CustomLabel** to **Country**. Then click on the green play button.

<img src="./media/image45.png" width="378" height="461" />

The results should look like this:

<img src="./media/image46.png" width="585" height="272" />

Save the visualization by clicking on the save icon. Set the **Title** to **HTTP ASN and Country** and then click on **Save**.

<img src="./media/image47.png" width="473" height="167" />

Switch back to the **Dashboard** tab. Then load the previously saved dashboard by clicking on the **Load Saved Dashboard** icon and then clicking on **HTTP Dashboard**.

<img src="./media/image48.png" width="687" height="142" />

Click on the **Add Visualization** icon.

<img src="./media/image49.png" width="395" height="73" />

Add the two new visualizations you just created by clicking on **HTTP ASN and Country** and **HTTP Naked IPs**.

<img src="./media/image50.png" width="339" height="227" />

Save the dashboard again by clicking on the save icon and then **Save**.

<img src="./media/image51.png" width="483" height="216" />

Now that the dashboard has been updated it is time to see what naked IP requests there are. The question is about logs from 2017. Change the search time to reflect this by clicking on the date picker and clicking on Absolute. Set the **From** date to **2017** and the **To** date to **2018**. Then click **Go**.

<img src="./media/image52.png" width="673" height="288" />

Minimize the time picker by clicking on the up arrow at the bottom.

<img src="./media/image53.png" width="100" height="36" />

With the current settings only the **HTTP Naked IPs** visualization reflects events that are related to naked IPs. Also, the question only pertains to events dealing with the IP address subnets of **192.168.2.0/24** and **10.0.0.0/24.** To change this set the search bar to "**tags:naked\_ip AND (source\_ip:\[192.168.2.0 TO 192.168.2.255\] OR source\_ip:\[10.0.0.0 TO 10.0.0.255\])**".

```bash
tags:naked_ip AND (source_ip:[192.168.2.0 TO 192.168.2.255] OR source_ip:[10.0.0.0 TO 10.0.0.255])
```

<img src="./media/image54.png" width="682" height="36" />

Looking at the **HTTP Naked IPs** visualization shows multiple Reverse DNS entries ending with **nflxvideo.net**. If you were to investigate this you would discover that this is traffic related to Netflix.

<img src="./media/image55.png" width="599" height="280" />

Also, looking at the **HTTP ASN and Country** visualization shows **Netflix Streaming Services Inc**. with a count of **2,544**. It also shows an ASN for **Fortinet Inc**. Since the question states these subnets are going out to the internet using a Fortinet firewall this is likely expected traffic. Click on **Netflix Streaming Services Inc.** and then change the filter to an exclusion by hovering over it and clicking the magnifying glass with a minus sign.

<img src="./media/image56.png" width="446" height="154" />

Do the same for **Fortinet Inc**.

<img src="./media/image57.png" width="609" height="150" />

At this point over 90% of the naked IP requests have been filtered out. However, 67 events may still be too many remaining to treat monitoring naked IP requests as effective. Yet more events can still be filtered. The highest remaining ASN is **Amazon.com, Inc**. Reverse DNS entries show these are related to **s3-1-w.amazonaws.com**. This is Amazon's Amazon Web Service (AWS) which is for cloud hosting. While www.amazon.com is likely trusted, AWS can be used for anything including attacks from adversaries.

Look at the raw events in the saved search section. Find the first event related to **s3-1w.amazonaws.com**. Notice, the **uri** is for **/kindle-wifi/wifistub.html**. In fact, it looks like these are the same for all **s3-1w.amazonaws.com** events. Expand the log and click on the magnifying glass with the minus sign to exclude events related to **/kindle-wifi/wifistub.html**.

<img src="./media/image58.png" width="582" height="25" />

The next highest remaining ASN is **Google Inc.** The problem is Google also has cloud hosting. Filtering out the ASN of **Google Inc**. could mask attacks from a Google hosted cloud server. Looking at the logs below all the Google events are related to a Reverse DNS entry ending in **1e100.net**.

<img src="./media/image59.png" width="319" height="66" />

**1e100.net** is a Google-owned domain used to identify their servers. It is not malicious and not associated with cloud hosted servers. Thus, it can be used to filter out these Google events. To do so update the search filter to include **-reverse\_dns:1e100.net** as follows:

```bash
tags:naked_ip AND (source_ip:[192.168.2.0 TO 192.168.2.255] OR source_ip:[10.0.0.0 TO 10.0.0.255]) -reverse_dns:1e100.net
```

<img src="./media/image60.png" width="682" height="30" />

Using ASN and reverse DNS filtering narrows the remaining naked IP requests to 3 web servers and 11 events. This is much easier for an analyst to handle.

**Answer**: **Netflix Streaming Services Inc.** and **Fortinet Inc.** were by far the two most used ASNs related to naked IP request. However, both Google and Amazon were common as well. In regards to these companies filtering can be done with either the ASN or fields such as **uri** and **reverse\_dns**. Because of cloud hosting filtering on **uri** and **reverse\_dns** may be a safer way to go. Using a combination of these results in **3 virtual\_hosts** with a total of **11** events that require investigation.

Bonus Challenge 1 â€“ Multiple sitesâ€¦ one destination IP?
=======================================================

Beginning on **March 10<sup>th</sup>, 2017** a system began to make many successful web connections to many web sites. However, they are all to the same destination IP and port.

1.  What is the IP of the system?

2.  What is the destination IP and port this system is connecting to?

3.  Is this malicious in nature?

Bonus Challenge 2 â€“ Malware URI vs Normal URI
=============================================

Logs recorded **before 2017** are from malware samples.

1.  What are the two sites with the longest URIs? dgyqimolcqm.cm and 5803.sindelclick.com

2.  Compare the highest URI length from 2012 to the highest URI length in 2017 from 192.168.2.0/24 (public internet). How much of a gap is there?


